import streamlit as st
# í˜ì´ì§€ ì„¤ì •ì€ ìŠ¤í¬ë¦½íŠ¸ ìµœìƒë‹¨ ë˜ëŠ” ë©”ì¸ í•¨ìˆ˜ ì‹œì‘ ë¶€ë¶„ì— ìœ„ì¹˜
st.set_page_config(page_title="ğŸ’¬ A2A Conversations", layout="wide", page_icon="ğŸ—£ï¸")

import asyncio
import json # JSON íŒŒì‹±/ë¤í”„ì— ì‚¬ìš©ë  ìˆ˜ ìˆìŒ (í˜„ì¬ ì½”ë“œì—ì„œëŠ” ì§ì ‘ ì‚¬ìš© ë¹ˆë„ ë‚®ìŒ)
import logging # ë¡œê¹… ëª¨ë“ˆ ì¶”ê°€
from typing import List, Tuple, Dict, Any, Optional # íƒ€ì… íŒíŒ…

# Streamlit í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë”°ë¥¸ ì„í¬íŠ¸ ê²½ë¡œ
from common.types import Message, TextPart, Task, Artifact # Task, Artifact ì¶”ê°€
from state.host_agent_service import (
    fetch_app_state_service,
    list_remote_agents_service,
    send_message_service,
    create_conversation_service,
    get_task_details_service, # íƒœìŠ¤í¬ ìƒì„¸ ì •ë³´ ì¡°íšŒ í•¨ìˆ˜ (host_agent_service.pyì— êµ¬í˜„ í•„ìš”)
)
from state.state import AppState # AppState ì„í¬íŠ¸

from dotenv import load_dotenv # .env íŒŒì¼ ë¡œë“œë¥¼ ìœ„í•´
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage # AIMessage íƒ€ì… íŒíŠ¸ ì¶”ê°€

logger = logging.getLogger(__name__) # ë¡œê±° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
load_dotenv(override=True) # OPENAI_API_KEY ë“±ì„ .env íŒŒì¼ì—ì„œ ë¡œë“œ

# --- ë¹„ë™ê¸° í—¬í¼ í•¨ìˆ˜ ---
async def _ainvoke_llm_safely(prompt_content: str) -> str:
    """LLMì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜."""
    if "chat_llm" not in st.session_state or st.session_state.chat_llm is None:
        logger.error("ChatLLMì´ ì„¸ì…˜ ìƒíƒœì— ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ê±°ë‚˜ Noneì…ë‹ˆë‹¤.")
        return "ì˜¤ë¥˜: LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    try:
        logger.debug(f"LLM prompt (first 100 chars): {prompt_content[:100]}...")
        response_message = await st.session_state.chat_llm.ainvoke([HumanMessage(content=prompt_content)])
        if hasattr(response_message, 'content') and isinstance(response_message.content, str):
            logger.debug(f"LLM response content (first 100 chars): {response_message.content[:100]}...")
            return response_message.content
        else:
            logger.error(f"LLM ì‘ë‹µ ê°ì²´ì— 'content' ì†ì„±ì´ ì—†ê±°ë‚˜ ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤. ì‘ë‹µ íƒ€ì…: {type(response_message)}")
            return "ì˜¤ë¥˜: LLMìœ¼ë¡œë¶€í„° ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ í˜•ì‹ì…ë‹ˆë‹¤."
    except Exception as e:
        logger.exception("LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ.")
        return f"LLM í†µì‹  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

async def orchestrate_servers_async(question: str, available_servers: List[str]) -> Tuple[List[str], str]:
    """
    LLMì—ê²Œ plan (í˜¸ì¶œí•  ì„œë²„ ëª©ë¡)ê³¼ prompt (ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ìŠ¹ì¸ ìš”ì²­ ë©”ì‹œì§€)ë¥¼
    ë¹„ë™ê¸°ì ìœ¼ë¡œ ìš”ì²­í•˜ê³  íŒŒì‹±í•©ë‹ˆë‹¤.
    """
    prompt_for_orchestrator = (
        "ë‹¹ì‹ ì€ A2A Orchestratorì…ë‹ˆë‹¤.\n"
        "ë‹¤ìŒ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ í˜¸ì¶œí•  ì„œë²„ ëª©ë¡(PLAN)ê³¼, ê·¸ ê³„íšì„ ì‚¬ìš©ìì—ê²Œ ìì—°ì–´ë¡œ ì„¤ëª…í•˜ë©° ìŠ¹ì¸ì„ ìš”ì²­í•  ë©”ì‹œì§€(PROMPT)ë¥¼ "
        "ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì¶œë ¥í•´ì£¼ì„¸ìš”. ê° í•­ëª©ì€ í•œ ì¤„ë¡œ ëª…í™•íˆ êµ¬ë¶„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:\n\n"
        "PLAN: http://server1.example.com, http://server2.example.com\n"
        "PROMPT: [ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ìì—°ì–´ ìŠ¹ì¸ ìš”ì²­ ë¬¸ì¥]\n\n"
        "ë§Œì•½ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ í˜¸ì¶œí•  ì„œë²„ê°€ í•„ìš” ì—†ê±°ë‚˜, ì§ì ‘ ë‹µë³€í•  ìˆ˜ ìˆë‹¤ë©´ PLANì—ëŠ” ë¹ˆ ê°’ì„, PROMPTì—ëŠ” ì§ì ‘ì ì¸ ë‹µë³€ì´ë‚˜ ì„¤ëª…ì„ í¬í•¨í•˜ì„¸ìš”.\n"
        f"ì§ˆë¬¸: {question}\n"
        f"ì‚¬ìš© ê°€ëŠ¥í•œ ì„œë²„ ëª©ë¡: {', '.join(available_servers) if available_servers else 'ì—†ìŒ'}\n"
    )
    
    raw_llm_response = await _ainvoke_llm_safely(prompt_for_orchestrator)
    
    parsed_plan: List[str] = []
    parsed_user_prompt: str = raw_llm_response # ê¸°ë³¸ì ìœ¼ë¡œ LLM ì‘ë‹µ ì „ì²´ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ì„¤ì •

    if "ì˜¤ë¥˜:" not in raw_llm_response: # LLM í˜¸ì¶œ ì„±ê³µ ì‹œì—ë§Œ íŒŒì‹± ì‹œë„
        prompt_found_in_response = False
        for line in raw_llm_response.splitlines():
            line_stripped = line.strip()
            if line_stripped.upper().startswith("PLAN:"):
                plan_str = line_stripped[len("PLAN:"):].strip()
                if plan_str: 
                    parsed_plan = [url.strip().rstrip("/") for url in plan_str.split(',') if url.strip()]
            elif line_stripped.upper().startswith("PROMPT:"):
                parsed_user_prompt = line_stripped[len("PROMPT:"):].strip()
                prompt_found_in_response = True
        
        if not prompt_found_in_response and not parsed_plan : # PLANë„ ì—†ê³  ëª…ì‹œì ì¸ PROMPTë„ ì—†ë‹¤ë©´, LLM ì‘ë‹µ ì „ì²´ë¥¼ ì‚¬ìš©
             pass # parsed_user_promptëŠ” ì´ë¯¸ raw_llm_responseë¡œ ì„¤ì •ë¨
        elif not prompt_found_in_response and parsed_plan: # PLANì€ ìˆì§€ë§Œ PROMPTê°€ ì—†ë‹¤ë©´ ê¸°ë³¸ ìŠ¹ì¸ ìš”ì²­ ë¬¸ì¥ ì‚¬ìš©
            parsed_user_prompt = "ì œì•ˆëœ ê³„íšì„ ìŠ¹ì¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (ì˜ˆ/ì•„ë‹ˆì˜¤)"
    else: # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
        parsed_user_prompt = "ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° LLM í˜¸ì¶œì— ì‹¤íŒ¨í•˜ì—¬ ê³„íšì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ì ‘ ë‹µë³€ì„ ì‹œë„í•©ë‹ˆë‹¤."

    logger.info(f"Orchestrator LLM Raw Response:\n{raw_llm_response}")
    logger.info(f"Parsed Plan: {parsed_plan}")
    logger.info(f"Parsed User Prompt: {parsed_user_prompt}")
    
    return parsed_plan, parsed_user_prompt

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” í•¨ìˆ˜ ---
def initialize_session_state_if_needed():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ì— í•„ìš”í•œ ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    if "chat_llm" not in st.session_state:
        try:
            st.session_state.chat_llm = ChatOpenAI(model="gpt-4o", temperature=0.0)
            logger.info("ChatOpenAI LLM (gpt-4o) initialized successfully.")
        except Exception as e:
            logger.error(f"Failed to initialize ChatOpenAI: {e}", exc_info=True)
            st.session_state.chat_llm = None
    if "histories" not in st.session_state:
        st.session_state.histories = {}
    if "current_conv_id" not in st.session_state:
        st.session_state.current_conv_id = None
    if "pending_plan" not in st.session_state:
        st.session_state.pending_plan = None
    if "pending_input" not in st.session_state:
        st.session_state.pending_input = None
    if "app_state_messages_loaded_for_conv" not in st.session_state:
        st.session_state.app_state_messages_loaded_for_conv = None
    
    if "active_polling_tasks" not in st.session_state:
        st.session_state.active_polling_tasks = {} 
    if "completed_task_results" not in st.session_state:
        st.session_state.completed_task_results = {}

initialize_session_state_if_needed()

# --- ë°ì´í„° ë¡œë”© í•¨ìˆ˜ ---
@st.cache_data(show_spinner="ëŒ€í™” ë‚´ìš© ë¡œë”© ì¤‘...", ttl=300)
def _fetch_app_state_for_conversation_cached(conv_id: str) -> Optional[AppState]:
    if not conv_id: return None
    logger.info(f"Fetching app state for conversation_id: {conv_id} (cacheable call)")
    return asyncio.run(fetch_app_state_service(conv_id))

def _load_and_prepare_chat_history(conv_id: str):
    """
    í˜„ì¬ ëŒ€í™” IDì— ëŒ€í•œ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ st.session_state.historiesì— ì¤€ë¹„í•©ë‹ˆë‹¤.
    st.rerun()ì„ ì§ì ‘ í˜¸ì¶œí•˜ì§€ ì•Šê³ , ìƒíƒœ ë³€ê²½ë§Œ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    """
    if not conv_id:
        logger.warning("_load_and_prepare_chat_history called with no conv_id.")
        return False # ë³€ê²½ ì—†ìŒ

    st.session_state.histories.setdefault(conv_id, [])
    history_changed = False
    
    needs_backend_load = (st.session_state.app_state_messages_loaded_for_conv != conv_id) or \
                         (not st.session_state.histories[conv_id] and \
                          st.session_state.app_state_messages_loaded_for_conv != conv_id)

    if needs_backend_load:
        logger.info(f"Loading messages from backend for conversation_id: {conv_id}")
        app_state_data = _fetch_app_state_for_conversation_cached(conv_id)
        
        backend_messages_formatted = []
        if app_state_data and app_state_data.messages:
            for msg_obj in app_state_data.messages:
                text_content = "".join(
                    str(data) for data, mime in msg_obj.content if mime == "text/plain"
                ).strip()
                if text_content or msg_obj.role :
                    backend_messages_formatted.append({"role": msg_obj.role or "unknown", "content": text_content})
        
        if st.session_state.histories[conv_id] != backend_messages_formatted: # ì‹¤ì œ ë³€ê²½ì´ ìˆì„ ë•Œë§Œ
            st.session_state.histories[conv_id] = backend_messages_formatted
            history_changed = True
        st.session_state.app_state_messages_loaded_for_conv = conv_id
        logger.info(f"Messages for conversation {conv_id} loaded/updated from backend. History has {len(backend_messages_formatted)} messages. Changed: {history_changed}")
    return history_changed


# --- í´ë§ ê´€ë ¨ í•¨ìˆ˜ ---
def extract_result_from_task(task: Optional[Task]) -> Optional[str]:
    """ì™„ë£Œëœ Task ê°ì²´ì—ì„œ ì‚¬ìš©ìê°€ ë³¼ ìµœì¢… ê²°ê³¼ ë¬¸ìì—´ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not task or not task.status or str(task.status.state).upper() != "COMPLETED":
        return None

    results_found = []
    if task.artifacts:
        for artifact in task.artifacts:
            if artifact.parts:
                for part in artifact.parts:
                    if isinstance(part, TextPart) and hasattr(part, 'text') and part.text:
                        results_found.append(part.text)
    
    if not results_found and task.history:
        for msg in reversed(task.history):
            if msg.role == "agent" and msg.parts:
                for part in msg.parts:
                    if isinstance(part, TextPart) and hasattr(part, 'text') and part.text:
                        results_found.append(part.text)
                if results_found: break 
    
    if not results_found and task.status.message and task.status.message.parts:
        for part in task.status.message.parts:
            if isinstance(part, TextPart) and hasattr(part, 'text') and part.text:
                results_found.append(part.text)
            
    if results_found:
        return "\n".join(results_found)

    logger.warning(f"Could not extract a clear result string from completed task {task.id}")
    return "ì‘ì—…ì€ ì™„ë£Œë˜ì—ˆìœ¼ë‚˜, í‘œì‹œí•  ëª…í™•í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."


async def check_and_update_polling_tasks(active_conv_id: str) -> bool:
    """í™œì„± í´ë§ íƒœìŠ¤í¬ ìƒíƒœ í™•ì¸ ë° UI ì—…ë°ì´íŠ¸. íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ ì‹œ True ë°˜í™˜."""
    if not st.session_state.active_polling_tasks:
        return False

    tasks_to_remove: List[str] = []
    history_updated = False
    current_history_list = st.session_state.histories.setdefault(active_conv_id, [])

    task_ids_to_poll = list(st.session_state.active_polling_tasks.keys())
    if not task_ids_to_poll:
        return False

    logger.debug(f"Polling for tasks: {task_ids_to_poll} in conv: {active_conv_id}")
    polling_coroutines = [get_task_details_service(task_id) for task_id in task_ids_to_poll]
    results = await asyncio.gather(*polling_coroutines, return_exceptions=True)

    for i, task_id in enumerate(task_ids_to_poll):
        task_result = results[i]
        original_question = st.session_state.active_polling_tasks.get(task_id, "ì•Œ ìˆ˜ ì—†ëŠ” ì§ˆë¬¸")

        if isinstance(task_result, Exception):
            logger.error(f"Error polling task {task_id}: {task_result}", exc_info=task_result)
            msg = f"ì—ì´ì „íŠ¸ ì‘ì—…(ID: {task_id[:8]}) ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(task_result)[:100]}"
            current_history_list.append({"role": "assistant", "content": msg})
            tasks_to_remove.append(task_id)
            history_updated = True
            continue

        task_detail: Optional[Task] = task_result
        if task_detail and task_detail.status:
            state_str = str(task_detail.status.state).upper()
            logger.debug(f"Task {task_id} (Q: '{original_question[:30]}...') polled state: {state_str}")

            if state_str == "COMPLETED":
                content = extract_result_from_task(task_detail) or "ì‘ì—… ì™„ë£Œ (ë‚´ìš© ì—†ìŒ)"
                logger.info(f"Task {task_id} COMPLETED. Result: {content[:100]}...")
                current_history_list.append({"role": "assistant", "content": content})
                st.session_state.completed_task_results[task_id] = content
                tasks_to_remove.append(task_id)
                history_updated = True
            elif state_str in ["FAILED", "ERROR", "CANCELLED"]:
                msg = f"ì—ì´ì „íŠ¸ ì‘ì—…(ID: {task_id[:8]})ì´ {state_str} ìƒíƒœë¡œ ì¢…ë£Œë¨ (Q: '{original_question}')."
                logger.error(msg)
                current_history_list.append({"role": "assistant", "content": msg})
                tasks_to_remove.append(task_id)
                history_updated = True
    
    if tasks_to_remove:
        for tid in tasks_to_remove:
            if tid in st.session_state.active_polling_tasks:
                del st.session_state.active_polling_tasks[tid]
        logger.info(f"Removed tasks from polling: {tasks_to_remove}")

    return history_updated

# --- ë©”ì¸ UI ë Œë”ë§ í•¨ìˆ˜ ---
def render_conversation_page():
    st.title("ğŸ’¬ Agent-to-Agent Orchestrated Chat")

    if st.session_state.get("chat_llm") is None:
        st.error("OpenAI LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ ë° ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

    logger.debug(f"[Page Render Start] current_conv_id: {st.session_state.get('current_conv_id')}, "
                 f"pending_plan: {st.session_state.get('pending_plan') is not None}, "
                 f"active_polling_tasks: {len(st.session_state.get('active_polling_tasks', {}))}")

    # --- ì‚¬ì´ë“œë°”: ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬ ---
    with st.sidebar:
        st.header("Conversations")
        if st.button("â• ìƒˆ ëŒ€í™” ì‹œì‘", key="new_conversation_sidebar_button_conv_page", use_container_width=True):
            logger.info("'ìƒˆ ëŒ€í™” ì‹œì‘' ë²„íŠ¼ í´ë¦­ë¨ (ì‚¬ì´ë“œë°”).")
            new_conv_object = asyncio.run(create_conversation_service())
            if new_conv_object and new_conv_object.conversation_id:
                new_id = new_conv_object.conversation_id
                st.session_state.histories[new_id] = []
                st.session_state.current_conv_id = new_id
                st.session_state.app_state_messages_loaded_for_conv = None 
                st.session_state.active_polling_tasks = {} 
                st.session_state.completed_task_results = {}
                logger.info(f"New conversation '{new_id}' created and selected. Rerunning.")
                st.rerun()
            else:
                st.error("ë°±ì—”ë“œì—ì„œ ìƒˆ ëŒ€í™” ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                logger.error("Failed to create new conversation or received invalid data from service.")

        conv_ids_sorted = sorted(st.session_state.get("histories", {}).keys(), reverse=True)

        for conv_id_key in conv_ids_sorted:
            display_label = f"ëŒ€í™” {conv_id_key[:8]}..."
            is_current = (conv_id_key == st.session_state.get("current_conv_id"))
            button_type = "primary" if is_current else "secondary"
            
            if st.button(display_label, key=f"select_conv_button_{conv_id_key}_conv_page", disabled=is_current, use_container_width=True, type=button_type):
                logger.info(f"Conversation '{conv_id_key}' selected from sidebar.")
                if st.session_state.current_conv_id != conv_id_key: # ì‹¤ì œ ID ë³€ê²½ ì‹œì—ë§Œ ìƒíƒœ ì´ˆê¸°í™” ë° rerun
                    st.session_state.current_conv_id = conv_id_key
                    st.session_state.app_state_messages_loaded_for_conv = None
                    st.session_state.active_polling_tasks = {} 
                    st.session_state.completed_task_results = {}
                    st.rerun()

    active_conv_id = st.session_state.get("current_conv_id")
    logger.debug(f"[Main Area Start] active_conv_id = {active_conv_id}")

    if not active_conv_id:
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ëŒ€í™”ë¥¼ ì„ íƒí•˜ê±°ë‚˜ 'ìƒˆ ëŒ€í™” ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
        st.stop()

    # --- ìë™ í´ë§ ì‹¤í–‰ ë° íˆìŠ¤í† ë¦¬ ë¡œë“œ/í‘œì‹œ ---
    # st.session_stateì— active_polling_tasksê°€ ìˆê³ , ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ ì‹¤í–‰
    if st.session_state.get("active_polling_tasks"): 
        logger.debug(f"Active polling tasks found: {list(st.session_state.active_polling_tasks.keys())}. Checking status.")
        history_updated_by_poll = asyncio.run(check_and_update_polling_tasks(active_conv_id))
        if history_updated_by_poll:
            logger.info("History was updated by polling. Rerunning to reflect.")
            st.rerun() # í´ë§ìœ¼ë¡œ íˆìŠ¤í† ë¦¬ ë³€ê²½ ì‹œ UI ìƒˆë¡œê³ ì¹¨

    # ë°±ì—”ë“œ ë©”ì‹œì§€ ë¡œë“œ (í•„ìš”ì‹œ). í´ë§ìœ¼ë¡œ íˆìŠ¤í† ë¦¬ê°€ ì—…ë°ì´íŠ¸ ë˜ì—ˆë‹¤ë©´, ì´ í•¨ìˆ˜ëŠ” ì¶”ê°€ ë¡œë“œë¥¼ ì•ˆí•  ìˆ˜ ìˆìŒ.
    # ë˜ëŠ”, ì´ í•¨ìˆ˜ê°€ ë¡œë“œí•œ ë‚´ìš©ê³¼ í´ë§ ê²°ê³¼ê°€ ë³‘í•©ë˜ì–´ì•¼ í•  ìˆ˜ë„ ìˆìŒ (í˜„ì¬ëŠ” êµì²´ ë°©ì‹)
    history_refreshed_by_load = _load_and_prepare_chat_history(active_conv_id)
    if history_refreshed_by_load and not st.session_state.active_polling_tasks: # í´ë§ì´ ì—†ê³ , ë¡œë“œë¡œ íˆìŠ¤í† ë¦¬ ë³€ê²½ ì‹œ
        logger.info("History refreshed by _load_and_prepare_chat_history. Rerunning.")
        st.rerun()
        
    active_chat_history = st.session_state.histories.get(active_conv_id, [])
    logger.debug(f"[After History Load & Poll] Messages for {active_conv_id}: {len(active_chat_history)}")

    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
    for msg_entry in active_chat_history:
        role = msg_entry.get("role", "unknown")
        avatar_icon = "ğŸ§‘â€ğŸ’»" if role == "user" else "ğŸ¤–"
        with st.chat_message(name=role, avatar=avatar_icon):
            st.markdown(msg_entry.get("content", ""))

    # --- ì‚¬ìš©ì ë©”ì‹œì§€ ì…ë ¥ ---
    logger.debug(f"[Before Chat Input] Rendering st.chat_input for {active_conv_id}")
    user_chat_input = st.chat_input(
        "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê±°ë‚˜, ì œì•ˆì— ëŒ€í•´ 'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆì˜¤'ë¡œ ë‹µí•´ì£¼ì„¸ìš”...", 
        key=f"chat_input_for_conv_{active_conv_id}_page" 
    )

    if user_chat_input:
        logger.info(f"User input for conv '{active_conv_id}': {user_chat_input}")
        
        current_history_list = st.session_state.histories.setdefault(active_conv_id, [])
        current_history_list.append({"role": "user", "content": user_chat_input})

        should_rerun_after_processing = False

        # --- 2ë‹¨ê³„ ì²˜ë¦¬: ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì œì•ˆì— ëŒ€í•œ ì‚¬ìš©ì ì‘ë‹µ ---
        if st.session_state.get("pending_plan") is not None:
            plan_to_execute = st.session_state.pending_plan
            original_question = st.session_state.pending_input

            st.session_state.pending_plan = None
            st.session_state.pending_input = None
            should_rerun_after_processing = True

            if "ì˜ˆ" in user_chat_input or "yes" in user_chat_input.lower():
                logger.info(f"User approved plan: {plan_to_execute} for Q: '{original_question}'")
                if not plan_to_execute:
                    msg_content = "ì•Œê² ìŠµë‹ˆë‹¤. ì´ì „ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì˜ ë‹µë³€ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”."
                    current_history_list.append({"role": "assistant", "content": msg_content})
                else:
                    for agent_url_target in plan_to_execute:
                        tool_msg_content = f"ğŸ’¡ **Tool Executing:** A2A Agent @ `{agent_url_target}` (Q: '{original_question}')"
                        current_history_list.append({"role": "assistant", "content": tool_msg_content})
                        
                        agent_message_to_send = Message(
                            role="user", parts=[TextPart(text=original_question)],
                            metadata={"conversation_id": active_conv_id, "target_agent_url": agent_url_target}
                        )
                        # send_message_serviceëŠ” íƒœìŠ¤í¬ IDë¥¼ ë°˜í™˜í•œë‹¤ê³  ê°€ì •
                        returned_task_id = asyncio.run(send_message_service(agent_message_to_send))
                        
                        if returned_task_id:
                            processing_feedback_msg = f"ì—ì´ì „íŠ¸ `{agent_url_target}`ì— ì‘ì—… ìš”ì²­ë¨ (Task ID: {returned_task_id[:8]}...). ê²°ê³¼ í™•ì¸ ì¤‘..."
                            current_history_list.append({"role": "assistant", "content": processing_feedback_msg})
                            st.session_state.active_polling_tasks[returned_task_id] = original_question # í´ë§ ëª©ë¡ì— ì¶”ê°€
                            logger.info(f"Task {returned_task_id} added to polling for Q: '{original_question}'")
                        else:
                            error_feedback_msg = f"ì—ì´ì „íŠ¸ `{agent_url_target}` ìš”ì²­ ì‹¤íŒ¨ (ID ë¯¸ìˆ˜ì‹ )."
                            current_history_list.append({"role": "assistant", "content": error_feedback_msg})
            else: # ê±°ë¶€
                logger.info(f"User rejected plan for Q: '{original_question}'. Using fallback LLM.")
                fallback_llm_response = asyncio.run(_ainvoke_llm_safely(original_question))
                current_history_list.append({"role": "assistant", "content": fallback_llm_response})
        
        # --- 1ë‹¨ê³„ ì²˜ë¦¬: ìƒˆë¡œìš´ ì‚¬ìš©ì ì§ˆë¬¸ (ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í˜¸ì¶œ) ---
        else:
            logger.info("Processing new user input with orchestrator.")
            should_rerun_after_processing = True
            remote_agents = asyncio.run(list_remote_agents_service())
            available_urls = [card.url.rstrip("/") for card in remote_agents if card and card.url]
            
            if not available_urls and (st.session_state.get("chat_llm") is None):
                 warning_message = "í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ì›ê²© ì—ì´ì „íŠ¸ë‚˜ ê¸°ë³¸ LLMì´ ì—†ì–´ ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                 current_history_list.append({"role": "assistant", "content": warning_message})
            elif not available_urls: # ì—ì´ì „íŠ¸ëŠ” ì—†ì§€ë§Œ ê¸°ë³¸ LLMì€ ìˆëŠ” ê²½ìš°
                logger.info("No remote agents available. Processing directly with default LLM.")
                direct_response = asyncio.run(_ainvoke_llm_safely(user_chat_input))
                current_history_list.append({"role": "assistant", "content": direct_response})
            else: # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í˜¸ì¶œ
                plan, approval_prompt = asyncio.run(
                    orchestrate_servers_async(user_chat_input, available_urls)
                )
                current_history_list.append({"role": "assistant", "content": approval_prompt})
                if plan: # ì‹¤í–‰í•  ê³„íšì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ë‹¤ìŒ ì…ë ¥ì„ ìœ„í•´ ì €ì¥
                    st.session_state.pending_plan = plan
                    st.session_state.pending_input = user_chat_input
                else: # ê³„íšì´ ì—†ëŠ” ê²½ìš° (LLMì´ ì§ì ‘ ë‹µë³€ ìƒì„± ë“±)
                    logger.info("Orchestrator returned no actionable plan. Approval message is considered the response.")
        
        if should_rerun_after_processing:
            st.rerun() # ëª¨ë“  ì…ë ¥ ì²˜ë¦¬ ë° íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ í›„ ìµœì¢…ì ìœ¼ë¡œ í•œ ë²ˆ rerun

# --- __main__ ---
if __name__ == "__main__":
    logging.basicConfig(
        level=logging.DEBUG, 
        format='%(asctime)s - %(levelname)s - %(name)s - [%(module)s.%(funcName)s:%(lineno)d] - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    logger.info("Streamlit Conversations Page starting session.")
    render_conversation_page()